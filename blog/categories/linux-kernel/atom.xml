<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux-kernel | Make Things Cool]]></title>
  <link href="http://markzhang.cn/blog/categories/linux-kernel/atom.xml" rel="self"/>
  <link href="http://markzhang.cn/"/>
  <updated>2016-01-06T09:49:52+08:00</updated>
  <id>http://markzhang.cn/</id>
  <author>
    <name><![CDATA[Mark Zhang]]></name>
    <email><![CDATA[super119@139.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Kernel suspend的逻辑]]></title>
    <link href="http://markzhang.cn/blog/2016/01/06/kernel-suspend-description/"/>
    <updated>2016-01-06T09:44:00+08:00</updated>
    <id>http://markzhang.cn/blog/2016/01/06/kernel-suspend-description</id>
    <content type="html"><![CDATA[<p>代码位于：power/suspend.c，从函数pm_suspend开始。</p>

<p>suspend的逻辑和driver相关的主要就是遍历所有device，并调用他们相关driver的suspend函数指针（包括该device的class，bus这些的suspend函数指针）。</p>

<!-- more -->


<p>至于device调用的先后次序是这样的：当我们执行device_add的时候，这些device会被加到一个dpm_list的list中，而且晚add的device会出现在list的靠前位置。也就是说，早add的device会晚一点被suspend，最后add的device会第一个被suspend。</p>

<p>resume的时候就正好反过来，最后add的device会第一个被resume。</p>

<p>而具体在suspend一个device的时候，会依次调用这些函数指针：device所属的power domain，device的type，device所属的class，device所属的bus。</p>

<p>在上述的逻辑之前，会有一个名为suspend_prepare的调用，在这里会freeze userspace（不schedule了就OK了），会freeze有freezable属性的workqueue。在上述逻辑之后，kernel就会转到arch层继续做suspend的动作。在这里就是每种arch，每种mach各不相同了。在这里一般就是针对具体的硬件，做state saving，clock gate，power gate，prepare cpu reset handler这样的一些动作了。</p>

<p>上面提到的有Freezable属性的这个workqueue比较实用，在suspend的时候会freeze，这样就会避免当suspend的时候，一些设备被关闭（或者clock被gate，或者power被gate），但是workqueue中还有work在运行从而导致一些问题。</p>

<p>跟了一下代码，这个system_freezable_wq主要就是在create workqueue的时候加上了一个FREEZABLE的flag，然后当suspend发生的时候，在没有开始真正suspend device之前，有一个suspend_prepare会调用suspend_freeze_processes，这样一路调用下来，最终会调用到freeze_workqueues_begin，在这里会将workqueue的maxactive设成0，这样workqueue里面的work就无法执行到了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kernel resume的逻辑]]></title>
    <link href="http://markzhang.cn/blog/2016/01/05/kernel-suspend-resume-description/"/>
    <updated>2016-01-05T15:59:00+08:00</updated>
    <id>http://markzhang.cn/blog/2016/01/05/kernel-suspend-resume-description</id>
    <content type="html"><![CDATA[<p>Resume的时候，其实最开始是从bootrom启动，再到uboot，然后再到kernel，到kernel的common code的时候，arch相关的代码都已经跑完，此时CPU已经起来了（其实CPU起来之后执行的第一句代码位于设定好的CPU reset handler指定地方的代码，一般这是一段汇编，在这里会去读取suspend结束的时候的一些系统状态，从而可以顺利的转到kernel层继续）。</p>

<!-- more -->


<p>所以到了kernel这边的时候，是从suspend结束时的下一条指令开始继续执行。具体就位于suspend.c的suspend_enter的下半部分代码。</p>

<p>在这里就可以看到，比如dpm_resume_noirq就是关键函数。在这里就会依次去调用所有device对应的driver的resume方法，顺序是先suspend的后被resume，后suspend的先resume。</p>

<p>结合之前说的suspend的逻辑，综合一下就是：</p>

<p>device_add的时候：</p>

<p>先被add的device，后被suspend，先被resume</p>

<p>后被add的device，先被suspend，后被resume</p>

<p>不过这不是绝对的，细节一点来说，这里面还分noirq的device resume list和irq enabled device resume list，具体的可以去看源代码。</p>

<p>然后suspend_enter函数就会被执行完成，按照之前suspend时候的调用栈，就会回到函数：suspend_devices_and_enter这个函数中调用suspend_enter的下一句继续执行，在这里又会做一些resume的工作，比如console resume就在这里。</p>

<p>最后就回到了调用suspend_devices_and_enter的函数enter_state，在这里会调用suspend_finish，这个函数里面就会调用到suspend_thaw_processes，这个函数就会thaw workqueues, thaw tasks，然后调用schedule开始进程调度，到这个时候userspace也就活过来了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[为什么linux的TTY登录程序getty/agetty一般都会设置一个-L的option？]]></title>
    <link href="http://markzhang.cn/blog/2016/01/03/getty-l/"/>
    <updated>2016-01-03T11:36:00+08:00</updated>
    <id>http://markzhang.cn/blog/2016/01/03/getty-l</id>
    <content type="html"><![CDATA[<p>-L option表示程序不等待DCD信号。DCD信号是串口通讯时的一个信号，表示Data Carrier Detected，一般是modem连通之后会发送回来的一个信号，表示数据载波OK，可以继续通信了。</p>

<!-- more -->


<p>但是，在现在的串口通信中，特别是嵌入式开发的时候，开发板上的串口，DCD这个pin一般都不会连线的，一般只连tx/rx/cts/rts四根线就OK了。</p>

<p>所以，如果getty或者agetty没有指定-L选项，那么open /dev/ttyS0就会block在那里，因为kernel会wait在一个waitqueue上，等待串口的DCD信号上来（通过中断）。</p>

<p>事实上，在getty/agetty程序中，指定-L option之后，代码中就会以NON_BLOCK的方式来打开/dev/ttyS0，这样kernel就不会去等待DCD信号了，从而打开串口就不会block了。</p>

<p>对于local的开发板上的串口来说，不连DCD线是合理的。因为这是local设备，不是要和远程的一个modem进行通信，所以需要DCD。本地的串口连上述那四根线就OK了。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[DMABUF/DMA Mapping/IOMMU/CMA/TTM/GEM/GART/SMMU]]></title>
    <link href="http://markzhang.cn/blog/2015/12/30/dmabuf-slash-dma-mapping-slash-iommu-slash-cma-slash-ttm-slash-gem-slash-gart-slash-smmu/"/>
    <updated>2015-12-30T09:24:00+08:00</updated>
    <id>http://markzhang.cn/blog/2015/12/30/dmabuf-slash-dma-mapping-slash-iommu-slash-cma-slash-ttm-slash-gem-slash-gart-slash-smmu</id>
    <content type="html"><![CDATA[<ol>
<li><p>DMABUF can be used as a wrapper to encapsulate other memory management frameworks. All these memory management framework(I mean mostly for graphics), buffer is the keypoint. DMABUF defines a standard buffer structure. So DMABUF can be used as a wrapper for TTM/GEM/Android ION&hellip; and etc. Notice DMABUF can&rsquo;t replace these things, cause it doesn&rsquo;t cover everything. E.g: DMABUF has no userspace interfaces, right now only kernel interfaces(can be used in device driver).</p></li>
<li><p>Kernel has DMA mapping API from origin. ARM defines IOMMU which can be used to connect scattered physical memory as a continuous region for devices which needs continue address to work(e.g: DMA). So IOMMU implementations &amp; CMA should work behind kernel DMA mapping API. E.g: dma_alloc_from_contiguous can be implemented by CMA; dma_alloc_coherent can be implemented by IOMMU or by the normal case(just call __get_free_pages). So for device drivers need dma buffers, we should use dma mapping APIs, not call iommu api directly.</p></li>
<li><p>For tegra, GART &amp; SMMU can be used to implement IOMMU apis.</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux graphics stack 理解]]></title>
    <link href="http://markzhang.cn/blog/2015/12/29/linux-graphics-stack/"/>
    <updated>2015-12-29T14:09:00+08:00</updated>
    <id>http://markzhang.cn/blog/2015/12/29/linux-graphics-stack</id>
    <content type="html"><![CDATA[<p>Display and mode setting: DRM. DRM defines connector/encoder/crtc to support display(including multi-monitor). And KMS makes mode setting happens in kernel space.</p>

<!-- more -->


<hr />

<p>2D acceleration: X server EXA/UXA extension. Vendor should write a X server driver which implements EXA or UXA(should be upstreamed). UXA is an enhancement design of EXA, proposed by intel, so GEM is used in UXA. DRM has no direct 2D acceleration interface definitions(e.g: memory copy/move, blit, color space conversion&hellip;), this is defined in EXA/UXA extensions. But vendor&rsquo;s X server driver will use drm functions, mostly is buffer manipulations.</p>

<p>3D acceleration: via Mesa. Mesa has 2 parts:</p>

<p>libGL.so &mdash; the implementation of OpenGL spec, the default OpenGL implementation library in Linux. This libGL.so translates OpenGL callings to Mesa-specified instructions.</p>

<p>DRI driver &mdash; the driver accepts the Mesa-specified instructions generated by libGL.so and call drm functions to get them accelerated in GPU. Should be upstreamed.</p>

<p>DRM: has userspace libdrm &amp; kernel space drm driver.</p>

<p>libdrm: Implemented drm APIs. Vendor can hook it&rsquo;s implementations as well as add more APIs which used by EXA/UXA driver(2D) or DRI driver(3D). That&rsquo;s why the build result of libdrm normally has 2 libraries(e.g: libdrm.so &amp; libdrm_intel.so). Vendor codes of libdrm should be upstreamed.</p>

<p>Kernel drm driver: Works under kernel drm framework, should be upstreamed. Cause vendor can add APIs in libdrm, so kernel drm driver can handle vendor specified ioctls.</p>

<p>GLX: X server extension of OpenGL. X server has DRI/DRI2 extensions as well.</p>

<p>They&rsquo;re working with libGL.so in Mesa. By these extensions, Mesa can do:</p>

<ol>
<li><p>Direct rendering: libGL.so in Mesa try to figure out whether DRI driver is ready in Mesa. If so, GLX returns some infos(such as window size, position&hellip;) or create off-screen buffers(via DRI/DRI2 X extension) for Mesa. The rendering is handled in Mesa&rsquo;s DRI driver &mdash; no relations with X server.</p></li>
<li><p>Indirect rendering: libGL.so in Mesa can&rsquo;t find out Mesa&rsquo;s DRI driver. So it queries whether DRI is available in X server. If so, it pass the Mesa-specified instructions which generated by libGL.so to X server. And GLX/DRI/DRI2 extensions of X server handles the rest things. Normally it&rsquo;s software rendering which implemented by Mesa as well. AIGLX(Accelerated Indirect GLX) seems has some solutions to accelerate this.</p></li>
</ol>


<p>So Mesa handles a lot of works in linux 3D graphics stack. And it has lots of relations with X server as well(GLX and DRI X extensions).</p>

<p>An important reason that why we involve so many components is, decouple the dependencies of all of these guys. According to this design, DRM is not related with X server so it works with other programs as well(e.g: Wayland). Mesa is the same.</p>
]]></content>
  </entry>
  
</feed>
