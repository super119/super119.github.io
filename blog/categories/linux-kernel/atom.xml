<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux-kernel | Make Things Cool]]></title>
  <link href="http://markzhang.cn/blog/categories/linux-kernel/atom.xml" rel="self"/>
  <link href="http://markzhang.cn/"/>
  <updated>2015-12-30T09:25:23+08:00</updated>
  <id>http://markzhang.cn/</id>
  <author>
    <name><![CDATA[Mark Zhang]]></name>
    <email><![CDATA[super119@139.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[DMABUF/DMA Mapping/IOMMU/CMA/TTM/GEM/GART/SMMU]]></title>
    <link href="http://markzhang.cn/blog/2015/12/30/dmabuf-slash-dma-mapping-slash-iommu-slash-cma-slash-ttm-slash-gem-slash-gart-slash-smmu/"/>
    <updated>2015-12-30T09:24:00+08:00</updated>
    <id>http://markzhang.cn/blog/2015/12/30/dmabuf-slash-dma-mapping-slash-iommu-slash-cma-slash-ttm-slash-gem-slash-gart-slash-smmu</id>
    <content type="html"><![CDATA[<ol>
<li><p>DMABUF can be used as a wrapper to encapsulate other memory management frameworks. All these memory management framework(I mean mostly for graphics), buffer is the keypoint. DMABUF defines a standard buffer structure. So DMABUF can be used as a wrapper for TTM/GEM/Android ION&hellip; and etc. Notice DMABUF can&rsquo;t replace these things, cause it doesn&rsquo;t cover everything. E.g: DMABUF has no userspace interfaces, right now only kernel interfaces(can be used in device driver).</p></li>
<li><p>Kernel has DMA mapping API from origin. ARM defines IOMMU which can be used to connect scattered physical memory as a continuous region for devices which needs continue address to work(e.g: DMA). So IOMMU implementations &amp; CMA should work behind kernel DMA mapping API. E.g: dma_alloc_from_contiguous can be implemented by CMA; dma_alloc_coherent can be implemented by IOMMU or by the normal case(just call __get_free_pages). So for device drivers need dma buffers, we should use dma mapping APIs, not call iommu api directly.</p></li>
<li><p>For tegra, GART &amp; SMMU can be used to implement IOMMU apis.</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux graphics stack 理解]]></title>
    <link href="http://markzhang.cn/blog/2015/12/29/linux-graphics-stack/"/>
    <updated>2015-12-29T14:09:00+08:00</updated>
    <id>http://markzhang.cn/blog/2015/12/29/linux-graphics-stack</id>
    <content type="html"><![CDATA[<p>Display and mode setting: DRM. DRM defines connector/encoder/crtc to support display(including multi-monitor). And KMS makes mode setting happens in kernel space.</p>

<!-- more -->


<hr />

<p>2D acceleration: X server EXA/UXA extension. Vendor should write a X server driver which implements EXA or UXA(should be upstreamed). UXA is an enhancement design of EXA, proposed by intel, so GEM is used in UXA. DRM has no direct 2D acceleration interface definitions(e.g: memory copy/move, blit, color space conversion&hellip;), this is defined in EXA/UXA extensions. But vendor&rsquo;s X server driver will use drm functions, mostly is buffer manipulations.</p>

<p>3D acceleration: via Mesa. Mesa has 2 parts:</p>

<p>libGL.so &mdash; the implementation of OpenGL spec, the default OpenGL implementation library in Linux. This libGL.so translates OpenGL callings to Mesa-specified instructions.</p>

<p>DRI driver &mdash; the driver accepts the Mesa-specified instructions generated by libGL.so and call drm functions to get them accelerated in GPU. Should be upstreamed.</p>

<p>DRM: has userspace libdrm &amp; kernel space drm driver.</p>

<p>libdrm: Implemented drm APIs. Vendor can hook it&rsquo;s implementations as well as add more APIs which used by EXA/UXA driver(2D) or DRI driver(3D). That&rsquo;s why the build result of libdrm normally has 2 libraries(e.g: libdrm.so &amp; libdrm_intel.so). Vendor codes of libdrm should be upstreamed.</p>

<p>Kernel drm driver: Works under kernel drm framework, should be upstreamed. Cause vendor can add APIs in libdrm, so kernel drm driver can handle vendor specified ioctls.</p>

<p>GLX: X server extension of OpenGL. X server has DRI/DRI2 extensions as well.</p>

<p>They&rsquo;re working with libGL.so in Mesa. By these extensions, Mesa can do:</p>

<ol>
<li><p>Direct rendering: libGL.so in Mesa try to figure out whether DRI driver is ready in Mesa. If so, GLX returns some infos(such as window size, position&hellip;) or create off-screen buffers(via DRI/DRI2 X extension) for Mesa. The rendering is handled in Mesa&rsquo;s DRI driver &mdash; no relations with X server.</p></li>
<li><p>Indirect rendering: libGL.so in Mesa can&rsquo;t find out Mesa&rsquo;s DRI driver. So it queries whether DRI is available in X server. If so, it pass the Mesa-specified instructions which generated by libGL.so to X server. And GLX/DRI/DRI2 extensions of X server handles the rest things. Normally it&rsquo;s software rendering which implemented by Mesa as well. AIGLX(Accelerated Indirect GLX) seems has some solutions to accelerate this.</p></li>
</ol>


<p>So Mesa handles a lot of works in linux 3D graphics stack. And it has lots of relations with X server as well(GLX and DRI X extensions).</p>

<p>An important reason that why we involve so many components is, decouple the dependencies of all of these guys. According to this design, DRM is not related with X server so it works with other programs as well(e.g: Wayland). Mesa is the same.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kernel: 怎么通过struct file *得到文件名]]></title>
    <link href="http://markzhang.cn/blog/2015/12/25/get-file-name-by-struct-file-star/"/>
    <updated>2015-12-25T10:54:00+08:00</updated>
    <id>http://markzhang.cn/blog/2015/12/25/get-file-name-by-struct-file-star</id>
    <content type="html"><![CDATA[<p>只是一个小Tip，代码如下：</p>

<!-- more -->


<p>``` cpp
char <em>tmp;
char </em>pathname;</p>

<p>path_get(&amp;file->f_path);</p>

<p>tmp = (char *)__get_free_page(GFP_TEMPORARY);
if (!tmp) {</p>

<pre><code>return -ENOMEM;
</code></pre>

<p>}</p>

<p>pathname = d_path(&amp;file->f_path, tmp, PAGE_SIZE);
path_put(&amp;file->f_path);</p>

<p>if (IS_ERR(pathname)) {</p>

<pre><code>free_page((unsigned long)tmp);
return PTR_ERR(pathname);
</code></pre>

<p>}</p>

<p>printk(KERN_WARNING &ldquo;File name: %s\n&rdquo;, pathname);
free_page((unsigned long)tmp);
```</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[KGDB配置使用]]></title>
    <link href="http://markzhang.cn/blog/2015/12/25/setup-kgdb/"/>
    <updated>2015-12-25T10:49:00+08:00</updated>
    <id>http://markzhang.cn/blog/2015/12/25/setup-kgdb</id>
    <content type="html"><![CDATA[<p>KGDB可以用于linux kernel的调试，具体的配置步骤是：</p>

<!-- more -->


<p>make menuconfig, under &ldquo;Kernel Hacking&rdquo;, enable &ldquo;KGDB&rdquo; &amp; &ldquo;KGDB over serial line&rdquo;. Enable &ldquo;compile kernel with debug info&rdquo;(CONFIG_DEBUG_INFO) &amp; &ldquo;enable frame pointer&rdquo;(CONFIG_FRAME_POINTER) options(both are under &ldquo;Kernel Hacking&rdquo;).</p>

<p>有关CONFIG_FRAME_POINTER这个option，需要在enable了ARCH_CONFIG_FRAME_POINTERS这个option之后才会出现在Kernel hacking下面。</p>

<p>这ARCH_CONFIG_FRAME_POINTERS在make menuconfig里面是找不到的。这个option是专门给其他人select用的。一般都是在ARCH config的时候会去select这个option</p>

<p>比如x86下，一般会enable这个option。而ARM则不会（可能是考虑到生成的kernel文件尺寸和性能）。</p>

<p>所以，在ARM下，可以编辑arch/arm/Kconfig，在CONFIG_ARM下面的一堆select的最后，添加上一句select CONFIG_ARCH_FRAME_POINTERS就可以了。</p>

<p>从实验结果来看，只要enable了CONFIG_DEBUG_INFO，基本上就差不多了，能看到代码了。</p>

<hr />

<p>Add kernel option: <code>kgdboc=ttyS0,38400 kgdbwait</code> into kernel command line.</p>

<p>根据我的实验，这里115200不行。在后面gdb设置target remote /dev/ttyS0的时候，会被告知最高支持到38400.</p>

<p>所以，这里如果115200不行，就改成38400。</p>

<hr />

<p>Start kernel, kernel will wait for connection from remote PC&rsquo;s gdb.</p>

<p>On remote machine, &ldquo;<gdb path> ./vmlinux&rdquo; &mdash; this &ldquo;vmlinux&rdquo; mostly is under the root directory of kernel(a big kernel image with debug infos, not the one under arch/arm/boot, that is stripped version).</p>

<p>需要注意的是，这里的GDB必须使用host是x86-64，target是ARM的那种GDB。我们PC上的gdb一般host和target都是x86-64。</p>

<p>这样的GDB可以在一些常见的toolchain网站找到，比如linaro。</p>

<hr />

<p>Enter gdb commands:</p>

<p><code>bash
set remotebaud 38400
target remote /dev/ttyS0
</code></p>

<hr />

<p>Done. You can list source codes, set breakpoints now. Continue the debugging after finished.</p>

<p>CAUTION: Close any other programs(normally it&rsquo;s minicom which we use to monitor serial outputs) which may occupy &ldquo;/dev/ttyS0&rdquo; before running gdb on remote machine. Otherwise gdb can&rsquo;t connect with target board.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kernel barrier/wmb/mb宏的作用]]></title>
    <link href="http://markzhang.cn/blog/2015/12/23/kernel-barrier-mb-wmb/"/>
    <updated>2015-12-23T10:28:00+08:00</updated>
    <id>http://markzhang.cn/blog/2015/12/23/kernel-barrier-mb-wmb</id>
    <content type="html"><![CDATA[<p>内存屏障主要解决的问题是编译器的优化和CPU的乱序执行。</p>

<p>编译器在优化的时候，生成的汇编指令可能和c语言程序的执行顺序不一样，在需要程序严格按照c语言顺序执行时，需要显式的告诉编译不需要优化，这在linux下是通过barrier()宏完成的，它依靠volidate关键字和memory关键字，前者告诉编译barrier()周围的指令不要被优化，后者作用是告诉编译器汇编代码会使内存里面的值更改，编译器应使用内存里的新值而非寄存器里保存的老值。</p>

<p>同样，CPU执行会通过乱序以提高性能。汇编里的指令不一定是按照我们看到的顺序执行的。linux中通过mb()系列宏来保证执行的顺序。具体做法是通过mfence/lfence指令（它们是奔4后引进的，早期x86没有）以及x86指令中带有串行特性的指令（这样的指令很多，例如linux中实现时用到的lock指令，I/O指令，操作控制寄存器、系统寄存器、调试寄存器的指令、iret指令等等）。简单的说，如果在程序某处插入了mb()/rmb()/wmb()宏，则宏之前的程序保证比宏之后的程序先执行，从而实现串行化。wmb的实现和barrier()类似，是因为在x86平台上，写内存的操作不会被乱序执行。</p>

<p>实际上在RSIC平台上，这些串行工作都有专门的指令由程序员显式的完成，比如在需要的地方调用串行指令，而不像x86上有这么多隐性的带有串行特性指令（例如lock指令）。所以在risc平台下工作的朋友通常对串行化操作理解的容易些。</p>
]]></content>
  </entry>
  
</feed>
